# Weather Data Pipeline with Apache Airflow

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.0+-red.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

Um pipeline de dados automatizado que coleta, processa e armazena dados climÃ¡ticos usando Apache Airflow. O projeto extrai dados meteorolÃ³gicos da API Visual Crossing Weather de forma semanal e organiza os dados em diferentes formatos CSV para anÃ¡lise.

## ğŸŒŸ Funcionalidades

- **ExtraÃ§Ã£o Automatizada**: Coleta dados climÃ¡ticos semanalmente
- **Processamento de Dados**: Separa dados em categorias (temperaturas e condiÃ§Ãµes)
- **OrganizaÃ§Ã£o Temporal**: Estrutura os dados por semanas
- **Monitoramento**: Interface visual do Airflow para acompanhar execuÃ§Ãµes
- **Tratamento de Erros**: Logs detalhados e recuperaÃ§Ã£o automÃ¡tica

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Visual        â”‚    â”‚   Apache         â”‚    â”‚   File System  â”‚
â”‚   Crossing API  â”‚â”€â”€â”€â–¶â”‚   Airflow        â”‚â”€â”€â”€â–¶â”‚   (CSV Files)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo do Pipeline:

1. **CriaÃ§Ã£o de DiretÃ³rio**: Cria pasta semanal com timestamp
2. **ExtraÃ§Ã£o de Dados**: Faz requisiÃ§Ã£o Ã  API para dados de 7 dias
3. **Processamento**: Separa dados em trÃªs arquivos:
   - `dados_brutos.csv`: Dataset completo
   - `temperaturas.csv`: Temperaturas mÃ­nima, mÃ©dia e mÃ¡xima
   - `condicoes.csv`: DescriÃ§Ã£o e Ã­cones do clima

## ğŸ“‹ PrÃ©-requisitos

- Python 3.8+
- Apache Airflow 2.0+
- Conta na [Visual Crossing Weather API](https://www.visualcrossing.com/weather-api)

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/weather-data-pipeline.git
cd weather-data-pipeline
```

### 2. Instale as dependÃªncias

```bash
pip install -r requirements.txt
```

### 3. Configure as variÃ¡veis de ambiente

```bash
# Copie o arquivo de exemplo
cp .env.example .env

# Edite o arquivo .env com sua API key
nano .env
```

### 4. Configure o Airflow

```bash
# Inicialize o banco de dados do Airflow
airflow db init

# Crie um usuÃ¡rio admin
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com
```

### 5. Copie a DAG para o diretÃ³rio do Airflow

```bash
cp dags/dados_climaticos.py $AIRFLOW_HOME/dags/
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
API_KEY=sua_chave_da_visual_crossing_api
AIRFLOW_HOME=/caminho/para/airflow
```

### ConfiguraÃ§Ã£o da DAG

A DAG estÃ¡ configurada para:
- **ExecuÃ§Ã£o**: Toda segunda-feira Ã  meia-noite
- **Cidade**: Boston (pode ser alterada no cÃ³digo)
- **PerÃ­odo**: Coleta dados de 7 dias
- **Fuso HorÃ¡rio**: UTC

## ğŸƒâ€â™‚ï¸ Executando

### 1. Inicie os serviÃ§os do Airflow

```bash
# Terminal 1 - Webserver
airflow webserver --port 8080

# Terminal 2 - Scheduler
airflow scheduler
```

### 2. Acesse a interface web

Abra seu navegador em: `http://localhost:8080`

### 3. Ative a DAG

- Encontre a DAG `dados_climaticos`
- Clique no toggle para ativÃ¡-la
- Monitore as execuÃ§Ãµes na interface

## ğŸ“‚ Estrutura do Projeto

```
weather-data-pipeline/
â”œâ”€â”€ README.md                 # Este arquivo
â”œâ”€â”€ requirements.txt          # DependÃªncias Python
â”œâ”€â”€ .env.example             # Template de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore               # Arquivos ignorados pelo Git
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ dados_climaticos.py  # DAG principal do Airflow
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ setup.md            # DocumentaÃ§Ã£o adicional
â””â”€â”€ tests/
    â””â”€â”€ test_dag.py         # Testes unitÃ¡rios
```

## ğŸ“Š Dados Coletados

### dados_brutos.csv
Dataset completo com todas as mÃ©tricas disponÃ­veis da API.

### temperaturas.csv
```csv
datetime,tempmin,temp,tempmax
2025-06-04,15.2,22.1,28.9
```

### condicoes.csv
```csv
datetime,description,icon
2025-06-04,Partly cloudy,partly-cloudy-day
```

## ğŸ› ï¸ PersonalizaÃ§Ã£o

### Alterar Cidade
```python
# Em dados_climaticos.py, linha ~45
city = 'Boston'  # Altere para sua cidade desejada
```

### Alterar FrequÃªncia
```python
# Em dados_climaticos.py, linha ~25
schedule_interval='0 0 * * 1',  # Cron expression
```

### Adicionar MÃ©tricas
```python
# Adicione mais colunas na extraÃ§Ã£o
dados[['datetime', 'humidity', 'windspeed']].to_csv(filepath + 'clima_extra.csv')
```

## ğŸ§ª Testes

Execute os testes unitÃ¡rios:

```bash
# Teste da DAG
python -m pytest tests/test_dag.py

# Teste manual da DAG
airflow dags test dados_climaticos 2025-06-04
```

## ğŸ“ Logs

Os logs do Airflow ficam em:
- **Interface Web**: `Admin > Logs`
- **Arquivos**: `$AIRFLOW_HOME/logs/dados_climaticos/`

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“œ LicenÃ§a

Este projeto estÃ¡ licenciado sob a MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ¯ PrÃ³ximos Passos

- [ ] Adicionar mais cidades
- [ ] Implementar alertas por email
- [ ] Criar dashboard de visualizaÃ§Ã£o
- [ ] Adicionar testes de integraÃ§Ã£o
- [ ] Dockerizar o projeto
- [ ] Implementar armazenamento em banco de dados

## ğŸ“ Contato

Seu Nome - [seu.email@exemplo.com](mailto:seu.email@exemplo.com)

Link do Projeto: [https://github.com/seu-usuario/weather-data-pipeline](https://github.com/seu-usuario/weather-data-pipeline)

---

â­ Se este projeto foi Ãºtil para vocÃª, considere dar uma estrela!
